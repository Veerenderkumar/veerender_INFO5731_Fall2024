{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Veerenderkumar/veerender_INFO5731_Fall2024/blob/main/kumar_veerender_Exercise_03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRwkJBn70nX"
      },
      "source": [
        "# **INFO5731 In-class Exercise 3**\n",
        "\n",
        "The purpose of this exercise is to explore various aspects of text analysis, including feature extraction, feature selection, and text similarity ranking.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of Friday, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n",
        "\n",
        "**Please check that the link you submitted can be opened and points to the correct assignment.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARqm7u6B70ne"
      },
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting **text classification or text mining task** and explain what kind of features might be useful for you to build the machine learning model. List your features and explain why these features might be helpful. You need to list at least five different types of features. **Your dataset must be text.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAZj4PHB70nf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Detecting fake news is an important task in today's information-driven world. The goal is to classify news articles as either fake or real. Fake news often contains misleading or sensational content, making it essential to identify such articles in the fight against misinformation.\n",
        "\n",
        "Features for Fake News Detection:\n",
        "TF-IDF (Term Frequency-Inverse Document Frequency)\n",
        "Why I need it: Fake news often uses attention-grabbing words. TF-IDF helps identify words that are overused in fake articles but less common across all articles.\n",
        "\n",
        "N-grams (Word Pairs or Triplets)\n",
        "Why I need it: Fake news tends to use word combinations like \"shocking discovery.\" N-grams help detect patterns in phrasing typical of misleading articles.\n",
        "\n",
        "Sentiment Analysis\n",
        "Why I need it: The goal of fake news is often to evoke strong emotions, whether positive or negative. Sentiment analysis can reveal exaggerated emotional tones.\n",
        "\n",
        "Readability Scores\n",
        "Why I need it: Real news is often more formal and complex, while fake news is typically written in simpler language to appeal to a wider audience. Readability measures help capture this difference.\n",
        "\n",
        "Part-of-Speech (POS) Tags\n",
        "Why I need it: Fake news may use more adjectives and adverbs to sound more dramatic. POS tags help detect grammatical patterns that distinguish real news from fake.\n",
        "\n"
      ],
      "metadata": {
        "id": "EfYc6tZ-LdWj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gfJ2qJh2Ladq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUjBE6C70nf"
      },
      "source": [
        "## Question 2 (10 Points)\n",
        "Write python code to extract these features you discussed above. You can collect a few sample text data for the feature extraction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "EoQX5s4O70nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aadec24f-0f92-43d3-e7ab-3cb71e95e782"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "TF-IDF Features:\n",
            "      alien      bill  breaking  caught  celebrity  championship  clone  \\\n",
            "0  0.386265  0.000000       0.0     0.0        0.0           0.0    0.0   \n",
            "1  0.000000  0.461149       0.0     0.0        0.0           0.0    0.0   \n",
            "2  0.000000  0.000000       0.0     0.5        0.5           0.0    0.0   \n",
            "3  0.000000  0.000000       0.0     0.0        0.0           0.0    0.0   \n",
            "4  0.000000  0.000000       0.0     0.0        0.0           0.0    0.0   \n",
            "\n",
            "     coffee     cures  dinosaur  ...  scientists  shocking     shows  sports  \\\n",
            "0  0.000000  0.000000       0.0  ...     0.32372  0.386265  0.000000     0.0   \n",
            "1  0.000000  0.000000       0.0  ...     0.00000  0.000000  0.000000     0.0   \n",
            "2  0.000000  0.000000       0.0  ...     0.00000  0.000000  0.000000     0.0   \n",
            "3  0.000000  0.000000       0.0  ...     0.00000  0.000000  0.000000     0.0   \n",
            "4  0.418767  0.418767       0.0  ...     0.00000  0.000000  0.418767     0.0   \n",
            "\n",
            "   stock     study   suggest  team   warming  wins  \n",
            "0    0.0  0.000000  0.000000   0.0  0.000000   0.0  \n",
            "1    0.0  0.000000  0.000000   0.0  0.000000   0.0  \n",
            "2    0.0  0.000000  0.000000   0.0  0.000000   0.0  \n",
            "3    0.0  0.000000  0.408248   0.0  0.408248   0.0  \n",
            "4    0.0  0.418767  0.000000   0.0  0.000000   0.0  \n",
            "\n",
            "[5 rows x 40 columns]\n",
            "\n",
            "Bigram Features:\n",
            "   alien life  breaking scientists  caught outrageous  celebrity caught  \\\n",
            "0           1                    0                  0                 0   \n",
            "1           0                    0                  0                 0   \n",
            "2           0                    0                  1                 1   \n",
            "3           0                    0                  0                 0   \n",
            "4           0                    0                  0                 0   \n",
            "\n",
            "   championship dramatic  clone dinosaur  coffee cures  cures diseases  \\\n",
            "0                      0               0             0               0   \n",
            "1                      0               0             0               0   \n",
            "2                      0               0             0               0   \n",
            "3                      0               0             0               0   \n",
            "4                      0               0             1               1   \n",
            "\n",
            "   discovery scientists  dramatic final  ...  scientists found  \\\n",
            "0                     1               0  ...                 1   \n",
            "1                     0               0  ...                 0   \n",
            "2                     0               0  ...                 0   \n",
            "3                     0               0  ...                 0   \n",
            "4                     0               0  ...                 0   \n",
            "\n",
            "   shocking discovery  shows coffee  sports team  stock market  study shows  \\\n",
            "0                   1             0            0             0            0   \n",
            "1                   0             0            0             0            0   \n",
            "2                   0             0            0             0            0   \n",
            "3                   0             0            0             0            0   \n",
            "4                   0             1            0             0            1   \n",
            "\n",
            "   suggest global  team wins  warming irreversible  wins championship  \n",
            "0               0          0                     0                  0  \n",
            "1               0          0                     0                  0  \n",
            "2               0          0                     0                  0  \n",
            "3               1          0                     1                  0  \n",
            "4               0          0                     0                  0  \n",
            "\n",
            "[5 rows x 34 columns]\n",
            "\n",
            "Sentiment Scores:\n",
            "                                                text  sentiment\n",
            "0  shocking discovery scientists found alien life...  -0.625000\n",
            "1              government passes new healthcare bill   0.136364\n",
            "2                celebrity caught outrageous scandal  -1.000000\n",
            "3  experts suggest global warming irreversible ef...   0.000000\n",
            "4              new study shows coffee cures diseases   0.136364\n",
            "\n",
            "POS Tag Counts:\n",
            "                                                text  \\\n",
            "0  shocking discovery scientists found alien life...   \n",
            "1              government passes new healthcare bill   \n",
            "2                celebrity caught outrageous scandal   \n",
            "3  experts suggest global warming irreversible ef...   \n",
            "4              new study shows coffee cures diseases   \n",
            "\n",
            "                                           pos_tags  \n",
            "0  {'VBG': 1, 'NN': 2, 'NNS': 2, 'VBD': 1, 'JJ': 1}  \n",
            "1                      {'NN': 3, 'VBZ': 1, 'JJ': 1}  \n",
            "2                      {'NN': 2, 'VBD': 1, 'JJ': 1}  \n",
            "3            {'NNS': 2, 'VBP': 1, 'JJ': 2, 'NN': 1}  \n",
            "4                      {'JJ': 1, 'NN': 2, 'NNS': 3}  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from textblob import TextBlob\n",
        "import nltk\n",
        "from nltk import pos_tag, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "data = {\n",
        "    'text': [\n",
        "        \"Shocking discovery! Scientists found alien life on Mars.\",\n",
        "        \"Government passes new healthcare bill.\",\n",
        "        \"Celebrity caught in outrageous scandal!\",\n",
        "        \"Experts suggest global warming has irreversible effects.\",\n",
        "        \"New study shows coffee cures all diseases.\",\n",
        "        \"Stock market hits all-time high.\",\n",
        "        \"Breaking: Scientists clone a dinosaur.\",\n",
        "        \"Sports team wins championship after dramatic final.\"\n",
        "    ],\n",
        "    'label': [1, 0, 1, 0, 1, 0, 1, 0]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "df['text'] = df['text'].str.lower().apply(nltk.word_tokenize)\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "df['text'] = df['text'].apply(lambda words: [w for w in words if w.isalpha() and w not in stop_words])\n",
        "\n",
        "df['text'] = df['text'].apply(lambda words: ' '.join(words))\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(df['text'])\n",
        "tfidf_features = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
        "print(\"\\nTF-IDF Features:\")\n",
        "print(tfidf_features.head())\n",
        "\n",
        "bigram_vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
        "bigram_matrix = bigram_vectorizer.fit_transform(df['text'])\n",
        "bigram_features = pd.DataFrame(bigram_matrix.toarray(), columns=bigram_vectorizer.get_feature_names_out())\n",
        "print(\"\\nBigram Features:\")\n",
        "print(bigram_features.head())\n",
        "\n",
        "df['sentiment'] = df['text'].apply(lambda text: TextBlob(text).sentiment.polarity)\n",
        "print(\"\\nSentiment Scores:\")\n",
        "print(df[['text', 'sentiment']].head())\n",
        "\n",
        "def pos_counts(text):\n",
        "    tags = pos_tag(word_tokenize(text))\n",
        "    pos_freq = nltk.FreqDist(tag for (word, tag) in tags)\n",
        "    return pos_freq\n",
        "\n",
        "df['pos_tags'] = df['text'].apply(pos_counts)\n",
        "print(\"\\nPOS Tag Counts:\")\n",
        "print(df[['text', 'pos_tags']].head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oSK4soH70nf"
      },
      "source": [
        "## Question 3 (10 points):\n",
        "Use any of the feature selection methods mentioned in this paper \"Deng, X., Li, Y., Weng, J., & Zhang, J. (2019). Feature selection for text classification: A review. Multimedia Tools & Applications, 78(3).\"\n",
        "\n",
        "Select the most important features you extracted above, rank the features based on their importance in the descending order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "2CRuXfV570ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1c533f4-4d3f-459a-d8a8-eb7801e2252e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top features ranked by Chi-Square scores:\n",
            "       Feature  Chi2_Score\n",
            "30  scientists    0.759277\n",
            "9     dinosaur    0.519708\n",
            "2     breaking    0.519708\n",
            "6        clone    0.519708\n",
            "20        high    0.500000\n",
            "29     scandal    0.500000\n",
            "21        hits    0.500000\n",
            "34       stock    0.500000\n",
            "27  outrageous    0.500000\n",
            "24      market    0.500000\n",
            "\n",
            "Top bigram features ranked by Chi-Square scores:\n",
            "                 Bigram  Chi2_Score\n",
            "0            alien life         1.0\n",
            "25   shocking discovery         1.0\n",
            "19       new healthcare         1.0\n",
            "20            new study         1.0\n",
            "21   outrageous scandal         1.0\n",
            "22           passes new         1.0\n",
            "23     scientists clone         1.0\n",
            "24     scientists found         1.0\n",
            "26         shows coffee         1.0\n",
            "1   breaking scientists         1.0\n",
            "\n",
            "Final dataset with selected features:\n",
            "                                                text  label_encoded  sentiment\n",
            "0  shocking discovery scientists found alien life...              1  -0.625000\n",
            "1              government passes new healthcare bill              0   0.136364\n",
            "2                celebrity caught outrageous scandal              1  -1.000000\n",
            "3  experts suggest global warming irreversible ef...              0   0.000000\n",
            "4              new study shows coffee cures diseases              1   0.136364\n",
            "5                             stock market hits high              0   0.160000\n",
            "6                 breaking scientists clone dinosaur              1   0.000000\n",
            "7       sports team wins championship dramatic final              0  -0.044444\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from textblob import TextBlob\n",
        "from sklearn.feature_selection import chi2\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import nltk\n",
        "from nltk import pos_tag, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "data = {\n",
        "    'text': [\n",
        "        \"Shocking discovery! Scientists found alien life on Mars.\",\n",
        "        \"Government passes new healthcare bill.\",\n",
        "        \"Celebrity caught in outrageous scandal!\",\n",
        "        \"Experts suggest global warming has irreversible effects.\",\n",
        "        \"New study shows coffee cures all diseases.\",\n",
        "        \"Stock market hits all-time high.\",\n",
        "        \"Breaking: Scientists clone a dinosaur.\",\n",
        "        \"Sports team wins championship after dramatic final.\"\n",
        "    ],\n",
        "    'label': [1, 0, 1, 0, 1, 0, 1, 0]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "df['text'] = df['text'].str.lower().apply(nltk.word_tokenize)\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "df['text'] = df['text'].apply(lambda words: [w for w in words if w.isalpha() and w not in stop_words])\n",
        "df['text'] = df['text'].apply(lambda words: ' '.join(words))\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(df['text'])\n",
        "tfidf_features = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "df['label_encoded'] = label_encoder.fit_transform(df['label'])\n",
        "\n",
        "chi2_scores, p_values = chi2(tfidf_matrix, df['label_encoded'])\n",
        "chi2_df = pd.DataFrame({'Feature': tfidf_vectorizer.get_feature_names_out(), 'Chi2_Score': chi2_scores})\n",
        "chi2_df = chi2_df.sort_values(by='Chi2_Score', ascending=False)\n",
        "\n",
        "print(\"\\nTop features ranked by Chi-Square scores:\")\n",
        "print(chi2_df.head(10))\n",
        "\n",
        "bigram_vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
        "bigram_matrix = bigram_vectorizer.fit_transform(df['text'])\n",
        "bigram_features = pd.DataFrame(bigram_matrix.toarray(), columns=bigram_vectorizer.get_feature_names_out())\n",
        "\n",
        "bigram_chi2_scores, bigram_p_values = chi2(bigram_matrix, df['label_encoded'])\n",
        "bigram_chi2_df = pd.DataFrame({'Bigram': bigram_vectorizer.get_feature_names_out(), 'Chi2_Score': bigram_chi2_scores})\n",
        "bigram_chi2_df = bigram_chi2_df.sort_values(by='Chi2_Score', ascending=False)\n",
        "\n",
        "print(\"\\nTop bigram features ranked by Chi-Square scores:\")\n",
        "print(bigram_chi2_df.head(10))\n",
        "\n",
        "df['sentiment'] = df['text'].apply(lambda text: TextBlob(text).sentiment.polarity)\n",
        "\n",
        "print(\"\\nFinal dataset with selected features:\")\n",
        "print(df[['text', 'label_encoded', 'sentiment']])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nZGAOwl70ng"
      },
      "source": [
        "## Question 4 (10 points):\n",
        "Write python code to rank the text based on text similarity. Based on the text data you used for question 2, design a query to match the most relevant docments. Please use the BERT model to represent both your query and the text data, then calculate the cosine similarity between the query and each text in your data. Rank the similary with descending order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "b4HoWK-i70ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74d56758-ed18-498b-cb2b-c263bcb51a6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ranked Documents based on Similarity to Query:\n",
            "\n",
            "                                                text  similarity_score\n",
            "0  Shocking discovery! Scientists found alien lif...          0.708499\n",
            "6             Breaking: Scientists clone a dinosaur.          0.512551\n",
            "1             Government passes new healthcare bill.          0.433043\n",
            "3  Experts suggest global warming has irreversibl...          0.426087\n",
            "5                   Stock market hits all-time high.          0.325972\n",
            "7  Sports team wins championship after dramatic f...          0.299732\n",
            "2            Celebrity caught in outrageous scandal!          0.287964\n",
            "4         New study shows coffee cures all diseases.          0.268384\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-62d38d3332c5>:31: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  ranked_df['similarity_score'] = cosine_scores[sorted_indexes].numpy()\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "\n",
        "data = {\n",
        "    'text': [\n",
        "        \"Shocking discovery! Scientists found alien life on Mars.\",\n",
        "        \"Government passes new healthcare bill.\",\n",
        "        \"Celebrity caught in outrageous scandal!\",\n",
        "        \"Experts suggest global warming has irreversible effects.\",\n",
        "        \"New study shows coffee cures all diseases.\",\n",
        "        \"Stock market hits all-time high.\",\n",
        "        \"Breaking: Scientists clone a dinosaur.\",\n",
        "        \"Sports team wins championship after dramatic final.\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "query = \"New scientific breakthrough in space exploration.\"\n",
        "\n",
        "query_embedding = model.encode(query, convert_to_tensor=True)\n",
        "text_embeddings = model.encode(df['text'].tolist(), convert_to_tensor=True)\n",
        "\n",
        "cosine_scores = util.pytorch_cos_sim(query_embedding, text_embeddings)[0]\n",
        "sorted_indexes = np.argsort(-cosine_scores)\n",
        "\n",
        "ranked_df = df.iloc[sorted_indexes]\n",
        "ranked_df['similarity_score'] = cosine_scores[sorted_indexes].numpy()\n",
        "\n",
        "print(\"Ranked Documents based on Similarity to Query:\\n\")\n",
        "print(ranked_df[['text', 'similarity_score']])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "VEs-OoDEhTW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on this exercise**\n",
        "\n",
        "Please provide your thoughts and feedback on the exercises you completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on extracting features from text data. What were the key concepts or techniques you found most beneficial in understanding the process?\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in completing this exercise?\n",
        "\n",
        "Relevance to Your Field of Study: How does this exercise relate to the field of NLP?\n",
        "\n",
        "**(Your submission will not be graded if this question is left unanswered)**\n",
        "\n"
      ],
      "metadata": {
        "id": "IUKC7suYhVl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Please write you answer here:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "CAq0DZWAhU9m"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}