{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Veerenderkumar/veerender_INFO5731_Fall2024/blob/main/INFO5731_Exercise_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DymRJbxDBCnf"
      },
      "source": [
        "# **INFO5731 In-class Exercise 2**\n",
        "\n",
        "The purpose of this exercise is to understand users' information needs, and then collect data from different sources for analysis by implementing web scraping using Python.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting research question (or practical question or something innovative) you have in mind, what kind of data should be collected to answer the question(s)? Specify the amount of data needed for analysis. Provide detailed steps for collecting and saving the data."
      ],
      "metadata": {
        "id": "FBKvD6O_TY6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "What are the key factors influencing the wealth accumulation of the top 1000 wealthiest people, and\n",
        "how do these factors vary across industries, regions, and age groups?\n",
        "Data Needed to Answer the Question: To answer this research question, you would need the following data points from the\n",
        "dataset:\n",
        "Name: The name of the individual.\n",
        "Net Worth: The total wealth of the individual in Billion USD.\n",
        "Country/Region: The geographical location or citizenship of the individual.\n",
        "Industry: The industry in which the individual primarily operates or has generated their wealth.\n",
        "Company: The company or business associated with their wealth (if applicable).\n",
        "These data points will allow you to analyze how wealth is distributed across different variables and what factors contribute to\n",
        "wealth accumulation.\n",
        "Data Required for Analysis: To conduct a meaningful analysis, the entire dataset containing the top 1000 wealthiest people\n",
        "should be collected. This dataset will provide sufficient variety across industries, countries, and other variables to draw\n",
        "significant insights.\n",
        "Detailed Steps for Collecting and Saving the Data:\n",
        "Step 1: Access the Dataset on Kaggle:\n",
        "Go to the Kaggle dataset page on https://www.kaggle.com/datasets/muhammadehsan02/top-1000-wealthiest-people-in-theworld. Download the dataset by clicking the \"Download\" button.\n",
        "Step 2: Load the Dataset into Google Colab: Upload the dataset to Google Colab.\n",
        "Step 3: Clean and Preprocess the Data:\n",
        "Ensure that missing values, if any, are handled properly (e.g., impute, drop, or flag). Make sure the columns are named\n",
        "correctly and are in a proper format for analysis.\n",
        "Step 4: Analyze the Data: You can conduct various analyses, such as:\n",
        "Distribution of Wealth by Country: Which countries have the highest number of wealthy individuals? Industry Influence: What\n",
        "industries are most prominent in wealth creation? Age and Wealth: What is the correlation between age and wealth?\n",
        "Step 5: Save the Cleaned Data for Further Analysis: After preprocessing, save the cleaned dataset to a CSV file for later use"
      ],
      "metadata": {
        "id": "cikVKDXdTbzE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "2e93582d-b6e0-43b7-d671-970934698aa0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-1-3ddf8319d551>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-3ddf8319d551>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    What are the key factors influencing the wealth accumulation of the top 1000 wealthiest people, and\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2 (10 Points)\n",
        "Write Python code to collect a dataset of 1000 samples related to the question discussed in Question 1."
      ],
      "metadata": {
        "id": "E9RqrlwdTfvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "UN505dabffrO",
        "outputId": "a9899c42-cd8b-46da-ec66-ba96309caf47"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f3164cf3-a792-46aa-b9f1-793a6aa076fe\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f3164cf3-a792-46aa-b9f1-793a6aa076fe\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Top_1000_wealthiest_people.csv to Top_1000_wealthiest_people (1).csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.colab import files\n",
        "from time import sleep\n",
        "\n",
        "df = pd.read_csv(list(uploaded.keys())[0])\n",
        "\n",
        "df_cleaned = df.dropna(subset=['Name', 'Country', 'Industry', 'Net Worth (in billions)', 'Company'])\n",
        "\n",
        "print(\"First 20 rows of the cleaned dataset:\")\n",
        "print(df_cleaned.head(20))\n",
        "\n",
        "sleep(5)\n",
        "\n",
        "df_cleaned.to_csv('cleaned_wealthiest_people.csv', index=False)\n",
        "\n",
        "print(\"\\nDownload the cleaned dataset:\")\n",
        "files.download('cleaned_wealthiest_people.csv')\n"
      ],
      "metadata": {
        "id": "4XvRknixTh1g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 815
        },
        "outputId": "4be05f3e-e4bb-458a-ed63-0bf25e5cb946"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 20 rows of the cleaned dataset:\n",
            "                 Name Country       Industry  Net Worth (in billions)  \\\n",
            "0          Rob Walton  Mexico        Finance                     8.50   \n",
            "1         Sergey Brin     USA     Automotive                    44.76   \n",
            "2       Steve Ballmer     USA  Manufacturing                    13.43   \n",
            "3       Mukesh Ambani     USA     Technology                   120.44   \n",
            "4          Jim Walton     USA        Fashion                   122.39   \n",
            "5         Sergey Brin     USA     Technology                    93.19   \n",
            "6   Michael Bloomberg     USA      Cosmetics                   117.96   \n",
            "7      Warren Buffett  France         Retail                    36.62   \n",
            "8         Carlos Slim     USA     Technology                    97.35   \n",
            "9          Larry Page     USA     Technology                    88.05   \n",
            "10         Rob Walton  France     Technology                    29.31   \n",
            "11       Alice Walton     USA     Technology                   167.09   \n",
            "12         Larry Page     USA     Technology                    60.75   \n",
            "13       Alice Walton     USA     Technology                   192.96   \n",
            "14      Larry Ellison     USA     Technology                    71.13   \n",
            "15    Bernard Arnault     USA  Manufacturing                    10.82   \n",
            "16         Larry Page     USA     Technology                    90.08   \n",
            "17      Larry Ellison     USA          Media                   167.65   \n",
            "18         Rob Walton     USA          Media                     6.94   \n",
            "19         Larry Page     USA     Technology                   184.80   \n",
            "\n",
            "                Company  \n",
            "0               Walmart  \n",
            "1                Google  \n",
            "2       Koch Industries  \n",
            "3                Google  \n",
            "4               Walmart  \n",
            "5               Walmart  \n",
            "6   Reliance Industries  \n",
            "7             Microsoft  \n",
            "8   Reliance Industries  \n",
            "9               Walmart  \n",
            "10   Berkshire Hathaway  \n",
            "11                 Zara  \n",
            "12  Reliance Industries  \n",
            "13               Oracle  \n",
            "14               Google  \n",
            "15               Google  \n",
            "16               Amazon  \n",
            "17      Koch Industries  \n",
            "18                 LVMH  \n",
            "19   Berkshire Hathaway  \n",
            "\n",
            "Download the cleaned dataset:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_7ad35c85-f971-4cd4-88f4-ebe017f2bab9\", \"cleaned_wealthiest_people.csv\", 45217)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03jb4GZsBkBS"
      },
      "source": [
        "## Question 3 (10 Points)\n",
        "Write Python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"XYZ\". The articles should be published in the last 10 years (2014-2024).\n",
        "\n",
        "The following information from the article needs to be collected:\n",
        "\n",
        "(1) Title of the article\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "YaGLbSHHB8Ej",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "431299a1-367e-4c0d-f8da-d15221698853"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting praw\n",
            "  Downloading praw-7.7.1-py3-none-any.whl.metadata (9.8 kB)\n",
            "Collecting prawcore<3,>=2.1 (from praw)\n",
            "  Downloading prawcore-2.4.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting update-checker>=0.18 (from praw)\n",
            "  Downloading update_checker-0.18.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.10/dist-packages (from praw) (1.8.0)\n",
            "Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from prawcore<3,>=2.1->praw) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2024.8.30)\n",
            "Downloading praw-7.7.1-py3-none-any.whl (191 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m191.0/191.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading prawcore-2.4.0-py3-none-any.whl (17 kB)\n",
            "Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
            "Installing collected packages: update-checker, prawcore, praw\n",
            "Successfully installed praw-7.7.1 prawcore-2.4.0 update-checker-0.18.0\n"
          ]
        }
      ],
      "source": [
        "# write your answer here\n",
        "!pip install praw\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "MtKskTzbCLaU"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "from google.colab import files\n",
        "\n",
        "API_ENDPOINT = 'https://api.crossref.org/works'\n",
        "SEARCH_TERM = 'tech'\n",
        "TOTAL_ARTICLES = 1000\n",
        "BEGIN_YEAR = 2014\n",
        "END_YEAR = 2024\n",
        "MAX_RETRIES = 5\n",
        "INITIAL_DELAY = 5\n",
        "\n",
        "def retrieve_articles(search_term, total_articles, begin_year, end_year):\n",
        "    articles_list = []\n",
        "    request_params = {\n",
        "        'query': search_term,\n",
        "        'rows': 100,\n",
        "        'filter': f'from-pub-date:{begin_year}-01-01,until-pub-date:{end_year}-12-31',\n",
        "        'select': 'title,container-title,published-print,author,abstract'\n",
        "    }\n",
        "\n",
        "    retry_attempt = 0\n",
        "    while len(articles_list) < total_articles:\n",
        "        try:\n",
        "            response = requests.get(API_ENDPOINT, params=request_params)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            if response.status_code == 429:\n",
        "                retry_attempt += 1\n",
        "                if retry_attempt > MAX_RETRIES:\n",
        "                    print(\"Rate limit exceeded. Stopping.\")\n",
        "                    break\n",
        "                wait_period = INITIAL_DELAY * (2 ** (retry_attempt - 1))\n",
        "                print(f\"Rate limit exceeded. Retrying in {wait_period} seconds...\")\n",
        "                time.sleep(wait_period)\n",
        "                continue\n",
        "\n",
        "            response_data = response.json()\n",
        "            articles_data = response_data.get('message', {}).get('items', [])\n",
        "\n",
        "            if not articles_data:\n",
        "                break\n",
        "\n",
        "            for item in articles_data:\n",
        "                article_entry = {\n",
        "                    'Title': item.get('title', [''])[0],\n",
        "                    'Venue': item.get('container-title', [''])[0],\n",
        "                    'Year': item.get('published-print', {}).get('date-parts', [[0]])[0][0],\n",
        "                    'Authors': ', '.join(\n",
        "                        author.get('family', '') + ' ' + author.get('given', '')\n",
        "                        for author in item.get('author', [])\n",
        "                    ),\n",
        "                    'Abstract': item.get('abstract', '')\n",
        "                }\n",
        "                articles_list.append(article_entry)\n",
        "\n",
        "                if len(articles_list) >= total_articles:\n",
        "                    break\n",
        "\n",
        "            request_params['offset'] = request_params.get('offset', 0) + 100\n",
        "\n",
        "        except requests.RequestException as e:\n",
        "            print(f\"Error retrieving data: {e}\")\n",
        "\n",
        "    return articles_list\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJDe71iLB616"
      },
      "source": [
        "## Question 4A (10 Points)\n",
        "Develop Python code to collect data from social media platforms like Reddit, Instagram, Twitter (formerly known as X), Facebook, or any other. Use hashtags, keywords, usernames, or user IDs to gather the data.\n",
        "\n",
        "\n",
        "\n",
        "Ensure that the collected data has more than four columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import praw\n",
        "import pandas as pd\n",
        "import warnings\n",
        "from google.colab import files\n",
        "from IPython.display import display\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module='praw')\n",
        "\n",
        "REDDIT_CLIENT_ID = 'O01Bu9MMEdpYrF9exIJCzg'\n",
        "REDDIT_CLIENT_SECRET = 'K0bQhSzmvcd2U0NmBUSQZx66o4dv7Q'\n",
        "REDDIT_USER_AGENT = 'script:Kanyoni:v2.7 (by u/Emergency-Apricot616)'\n",
        "\n",
        "reddit = praw.Reddit(client_id=REDDIT_CLIENT_ID,\n",
        "                     client_secret=REDDIT_CLIENT_SECRET,\n",
        "                     user_agent=REDDIT_USER_AGENT)\n",
        "\n",
        "def reddit_posts_fetcher(subreddit, keyword, num_posts=100):\n",
        "    posts_list = []\n",
        "    subreddit_instance = reddit.subreddit(subreddit)\n",
        "\n",
        "    for submission in subreddit_instance.search(keyword, limit=num_posts):\n",
        "        post = {\n",
        "            'Title': submission.title,\n",
        "            'Score': submission.score,\n",
        "            'ID': submission.id,\n",
        "            'URL': submission.url,\n",
        "            'Selftext': submission.selftext,\n",
        "            'Created_UTC': submission.created_utc\n",
        "        }\n",
        "        posts_list.append(post)\n",
        "\n",
        "    return posts_list\n"
      ],
      "metadata": {
        "id": "LQHii1S-emvc"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55W9AMdXCSpV"
      },
      "source": [
        "## Question 4B (10 Points)\n",
        "If you encounter challenges with Question-4 web scraping using Python, employ any online tools such as ParseHub or Octoparse for data extraction. Introduce the selected tool, outline the steps for web scraping, and showcase the final output in formats like CSV or Excel.\n",
        "\n",
        "\n",
        "\n",
        "Upload a document (Word or PDF File) in any shared storage (preferably UNT OneDrive) and add the publicly accessible link in the below code cell.\n",
        "\n",
        "Please only choose one option for question 4. If you do both options, we will grade only the first one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I57NXsauCec2"
      },
      "outputs": [],
      "source": [
        "# write your answer here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "sZOhks1dXWEe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on Web Scraping and Data Collection**\n",
        "\n",
        "\n",
        "\n",
        "Please share your thoughts and feedback on the web scraping and data collection exercises you have completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on web scraping tasks. What were the key concepts or techniques you found most beneficial in understanding the process of extracting data from various online sources?\n",
        "\n",
        "\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in collecting data from certain websites, and how did you overcome them? If you opted for the non-coding option, share your experience with the chosen tool.\n",
        "\n",
        "\n",
        "\n",
        "Relevance to Your Field of Study: How might the ability to gather and analyze data from online sources enhance your work or research?\n",
        "\n",
        "**(no grading of your submission if this question is left unanswered)**"
      ],
      "metadata": {
        "id": "eqmHVEwaWhbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Write your response here.\n",
        "'''"
      ],
      "metadata": {
        "id": "akAVJn9YBTQT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "FBKvD6O_TY6e",
        "E9RqrlwdTfvl",
        "03jb4GZsBkBS",
        "jJDe71iLB616",
        "55W9AMdXCSpV",
        "4ulBZ6yhCi9F",
        "6SmvS7nSfbj8",
        "sZOhks1dXWEe"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}